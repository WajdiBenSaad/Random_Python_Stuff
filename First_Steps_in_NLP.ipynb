{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction :\nData comes in various forms and types, numbers, timestamps, dates, images... and text is one of these forms that differs in terms of how we get text data, the shape and nature of this text, and especially what we can do with it and how.\nIn this notebook, I will go through some basic operations and functions to see what kind of fun we can have with textual data.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## Importing spaCy lirary for NLP :\nimport spacy\n## We are going to analyze text written in the English language, let's load it also\nnlp = spacy.load('en')\n","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## let's try to input a small text to see what we can do :\ndoc = nlp(\"Football is not my favorite sport\")\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's split our text into tokens:\nfor token in doc:\n    print(token)","execution_count":3,"outputs":[{"output_type":"stream","text":"Football\nis\nnot\nmy\nfavorite\nsport\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"span=doc[1:3]\nspan.text","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'is not'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Preprocessing\nBefore we start doing complex things, we need to do some transformations to the data we have. these transformations a bit different than the transformations applied to non-text data. we will start by lemmatizing our words.\nWe need to do this because words do not all have the same usage and meaing, especially stopwords for instance.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\nfor token in doc:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Token \t\tLemma \t\tStopword\n----------------------------------------\nFootball\t\tfootball\t\tFalse\nis\t\tbe\t\tTrue\nnot\t\tnot\t\tTrue\nmy\t\t-PRON-\t\tTrue\nfavorite\t\tfavorite\t\tFalse\nsport\t\tsport\t\tFalse\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"There are various other transformations that we could apply on our text, like transforming to lower case, matching words with "},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting attr='LOWER' will match the phrases on lowercased text. This provides case insensitive matching.\n\nNext you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model."},{"metadata":{"trusted":true},"cell_type":"code","source":"terms = ['Player', 'goal', 'ball', 'game']\npatterns = [nlp(text) for text in terms]\nmatcher.add(\"TerminologyList\", None, *patterns)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_doc = nlp(\"I really am looking forward to the game tonight \"\n               \"They missed the goal 5 times\"\n               \"Just don't drop the ball\") \nmatches = matcher(text_doc)\nprint(matches)","execution_count":8,"outputs":[{"output_type":"stream","text":"[(3766102292120407359, 7, 8), (3766102292120407359, 12, 13), (3766102292120407359, 19, 20)]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The matches here are a tuple of the match id and the positions of the start and end of the phrase.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"match_id, start, end = matches[1]\nprint(nlp.vocab.strings[match_id], text_doc[start:end])","execution_count":9,"outputs":[{"output_type":"stream","text":"TerminologyList goal\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Text Classification\nText classification is one of the most common NLP tasks. We will use SpaCy to analyze and build understanding of a peice of text, then we will use it to detect if the text is spam or not. This should be interesting :"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Loading the spam data, we took this from a Kaggle competition dataset\n# ham is the label for non-spam messages\nspam = pd.read_csv('../input/nlp-course/spam.csv')\nspam.head(10)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"  label                                               text\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...\n5  spam  FreeMsg Hey there darling it's been 3 week's n...\n6   ham  Even my brother is not like to speak with me. ...\n7   ham  As per your request 'Melle Melle (Oru Minnamin...\n8  spam  WINNER!! As a valued network customer you have...\n9  spam  Had your mobile 11 months or more? U R entitle...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spam</td>\n      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>spam</td>\n      <td>WINNER!! As a valued network customer you have...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>spam</td>\n      <td>Had your mobile 11 months or more? U R entitle...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Building a Bag of Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\n# Create an empty model\nnlp = spacy.blank(\"en\")\n\n# Create the TextCategorizer with exclusive classes and \"bow\" architecture\ntextcat = nlp.create_pipe(\n              \"textcat\",\n              config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"bow\"})\n\n# Add the TextCategorizer to the empty model\nnlp.add_pipe(textcat)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add labels to text classifier\ntextcat.add_label(\"ham\")\ntextcat.add_label(\"spam\")","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Training a Text Categorizer Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' \nNext, you'll convert the labels in the data to the form TextCategorizer requires. For each document, \nyou'll create a dictionary of boolean values for each class.\nFor example, if a text is \"ham\", we need a dictionary {'ham': True, 'spam': False}. \nThe model is looking for these labels inside another dictionary with the key 'cats'.\n'''\ntrain_texts = spam['text'].values\ntrain_labels = [{'cats': {'ham': label == 'ham',\n                          'spam': label == 'spam'}} \n                for label in spam['label']]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = list(zip(train_texts, train_labels))\ntrain_data[:3]","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n  {'cats': {'ham': True, 'spam': False}}),\n ('Ok lar... Joking wif u oni...', {'cats': {'ham': True, 'spam': False}}),\n (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n  {'cats': {'ham': False, 'spam': True}})]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.util import minibatch\n\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\n# Create the batch generator with batch size = 8\nbatches = minibatch(train_data, size=8)\n# Iterate through minibatches\nfor batch in batches:\n    # Each batch is a list of (text, label) but we need to\n    # send separate lists for texts and labels to update().\n    # This is a quick way to split a list of tuples into lists\n    texts, labels = zip(*batch)\n    nlp.update(texts, labels, sgd=optimizer)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(1)\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\nlosses = {}\nfor epoch in range(10):\n    random.shuffle(train_data)\n    # Create the batch generator with batch size = 8\n    batches = minibatch(train_data, size=8)\n    # Iterate through minibatches\n    for batch in batches:\n        # Each batch is a list of (text, label) but we need to\n        # send separate lists for texts and labels to update().\n        # This is a quick way to split a list of tuples into lists\n        texts, labels = zip(*batch)\n        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n    print(losses)","execution_count":16,"outputs":[{"output_type":"stream","text":"{'textcat': 0.4348172624983704}\n{'textcat': 0.6518694226331547}\n{'textcat': 0.7900564276918112}\n{'textcat': 0.8785885196629231}\n{'textcat': 0.9360322702493176}\n{'textcat': 0.9741643182592479}\n{'textcat': 1.0029479640338683}\n{'textcat': 1.022063153248012}\n{'textcat': 1.0369556128306696}\n{'textcat': 1.0472958829307644}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' \nNow that you have a trained model, you can make predictions with the predict() method. \nThe input text needs to be tokenized with nlp.tokenizer. \nThen you pass the tokens to the predict method which returns scores. \nThe scores are the probability the input text belongs to the classes.\n''' \ntexts = [\"Are you ready for the tea party????? It's gonna be wild\",\n         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\ndocs = [nlp.tokenizer(text) for text in texts]\n    \n# Use textcat to get the scores for each doc\ntextcat = nlp.get_pipe('textcat')\nscores, _ = textcat.predict(docs)\n\nprint(scores)","execution_count":17,"outputs":[{"output_type":"stream","text":"[[9.9994671e-01 5.3246935e-05]\n [1.2245358e-02 9.8775464e-01]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From the scores, find the label with the highest score/probability\npredicted_labels = scores.argmax(axis=1)\nprint([textcat.labels[label] for label in predicted_labels])","execution_count":18,"outputs":[{"output_type":"stream","text":"['ham', 'spam']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"   © www.wajdibensaad.com "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}