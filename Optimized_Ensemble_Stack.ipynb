{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>triceps</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>dpf</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnancies  glucose  diastolic  triceps  insulin   bmi    dpf  age  \\\n",
       "0            6      148         72       35        0  33.6  0.627   50   \n",
       "1            1       85         66       29        0  26.6  0.351   31   \n",
       "2            8      183         64        0        0  23.3  0.672   32   \n",
       "3            1       89         66       23       94  28.1  0.167   21   \n",
       "4            0      137         40       35      168  43.1  2.288   33   \n",
       "\n",
       "   diabetes  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the Data set\n",
    "df = pd.read_csv('data/diabetes_data.csv')\n",
    "\n",
    "#Checking the data structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the target variables:\n",
    "X = df.drop(columns = ['diabetes'])\n",
    "y = df['diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'brute', 'leaf_size': 64, 'n_neighbors': 11, 'p': 1, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "seed=123\n",
    "#KNN Model:\n",
    "#intiate the KNN Model:\n",
    "knn = KNeighborsClassifier()\n",
    "#Listing the parameters to best tested :\n",
    "params_knn = {'n_neighbors': sp_randint(1,80),\n",
    "              'algorithm' : ( 'ball_tree', 'kd_tree', 'brute'),\n",
    "              'weights':['distance','uniform'],\n",
    "              'p':[1,2,3,4],\n",
    "             'leaf_size': sp_randint(1,100)}\n",
    "#Randomized Grid Search to get the best parameters for the model\n",
    "\n",
    "n_iter_search = 20\n",
    "knn_gs = RandomizedSearchCV(knn, param_distributions=params_knn,\n",
    "                                   n_iter=n_iter_search, cv=5,random_state=998766)\n",
    "\n",
    "\n",
    " \n",
    "#fitting the model to training data\n",
    "knn_gs.fit(X_train, y_train)\n",
    "\n",
    "#saving the best model:\n",
    "knn_best = knn_gs.best_estimator_\n",
    "\n",
    "#checking the best perameters values:\n",
    "print(knn_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn roc_auc: 0.7134567901234568\n"
     ]
    }
   ],
   "source": [
    "y_pred_knn=knn_best.predict(X_test)\n",
    "print('knn roc_auc: {}'.format(roc_auc_score(y_test,y_pred_knn)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'max_leaf_nodes': 364, 'n_estimators': 442}\n",
      "rf roc_auc: 0.7101234567901235\n"
     ]
    }
   ],
   "source": [
    "# Random Forest:\n",
    "#Initiate the Random Forest Model:\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Listing the parameters to best tested :\n",
    "params_rf = {'n_estimators': sp_randint(50,700), \n",
    "             'max_leaf_nodes': sp_randint(20,600),\n",
    "            'bootstrap':[True,False]}\n",
    "\n",
    "#Grid Search to get the best parameters for the model\n",
    "n_iter_search = 20\n",
    "rf_gs = RandomizedSearchCV(rf, param_distributions=params_rf,\n",
    "                                   n_iter=n_iter_search, cv=5,random_state=998766)\n",
    "\n",
    "#fitting the model to training data\n",
    "rf_gs.fit(X_train, y_train)\n",
    "\n",
    "#saving the best model:\n",
    "rf_best = rf_gs.best_estimator_\n",
    "\n",
    "#check best n_estimators value\n",
    "print(rf_gs.best_params_)\n",
    "\n",
    "y_pred_rf=rf_best.predict(X_test)\n",
    "print('rf roc_auc: {}'.format(roc_auc_score(y_test,y_pred_rf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 2, 'max_iter': 50}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression:\n",
    "#create a new logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "#Listing the parameters to best tested :\n",
    "params_lg = {'C': [0.01, 0.1, 1,2], 'max_iter':[50,100,500]}\n",
    "#Grid Search to get the best parameters for the model\n",
    "\n",
    "lg_gs = GridSearchCV(log_reg, params_lg, cv=15)\n",
    "\n",
    "#fitting the model to training data\n",
    "lg_gs.fit(X_train, y_train)\n",
    "#saving the best model:\n",
    "lg_best = lg_gs.best_estimator_\n",
    "\n",
    "#check best n_estimators value\n",
    "print(lg_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#Extra Tree:\n",
    "#create a new Extra tree model\n",
    "ET = ExtraTreesClassifier()\n",
    "#Listing the parameters to best tested :\n",
    "params_ET = {'n_estimators':[100,500],'max_depth':[ 5,10,20,100]}\n",
    "#Grid Search to get the best parameters for the model\n",
    "ET_gs = GridSearchCV(ET, params_ET, cv=15)\n",
    "\n",
    "#fitting the model to training data\n",
    "ET_gs.fit(X_train, y_train)\n",
    "#saving the best model:\n",
    "ET_best = ET_gs.best_estimator_\n",
    "\n",
    "#check best n_estimators value\n",
    "print(ET_gs.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "#Adaboost\n",
    "#create a new Extra tree model\n",
    "Ada = AdaBoostClassifier()\n",
    "#Listing the parameters to best tested :\n",
    "params_ada = {'n_estimators':[100,500],'learning_rate':[ 0.001,0.01]}\n",
    "#Grid Search to get the best parameters for the model\n",
    "Ada_gs = GridSearchCV(Ada, params_ada, cv=15)\n",
    "\n",
    "#fitting the model to training data\n",
    "Ada_gs.fit(X_train, y_train)\n",
    "\n",
    "#saving the best model:\n",
    "Ada_best = Ada_gs.best_estimator_\n",
    "\n",
    "#check best n_estimators value\n",
    "print(Ada_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_fraction': 0.22, 'learning_rate': 0.01, 'n_estimators': 500, 'num_leaves': 20}\n"
     ]
    }
   ],
   "source": [
    "#LightGBM\n",
    "#create a new Extra tree model\n",
    "lgb = LGBMClassifier( metric='auc',max_depth= -1)\n",
    "#Listing the parameters to best tested :\n",
    "params_lgb = {'n_estimators':[500,1000],'learning_rate':[ 0.0089,0.01], \n",
    "              'num_leaves': [10,20,50],    'feature_fraction': [0.22,0.033,0.04]}\n",
    "#Grid Search to get the best parameters for the model\n",
    "lgb_gs = GridSearchCV(lgb, params_lgb, cv=15)\n",
    "\n",
    "#fitting the model to training data\n",
    "lgb_gs.fit(X_train, y_train)\n",
    "\n",
    "#saving the best model:\n",
    "lgb_best = lgb_gs.best_estimator_\n",
    "\n",
    "#check best n_estimators value\n",
    "print(lgb_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn: 0.7532467532467533\n",
      "rf: 0.7489177489177489\n",
      "log_reg: 0.7748917748917749\n",
      "ExtraTree: 0.7316017316017316\n",
      "AdaBoost: 0.7532467532467533\n",
      "LightGBM: 0.70995670995671\n"
     ]
    }
   ],
   "source": [
    "#Printing the accuracy scores of the models\n",
    "\n",
    "print('knn: {}'.format(knn_best.score(X_test, y_test)))\n",
    "print('rf: {}'.format(rf_best.score(X_test, y_test)))\n",
    "print('log_reg: {}'.format(lg_best.score(X_test, y_test)))\n",
    "print('ExtraTree: {}'.format(ET_best.score(X_test, y_test)))\n",
    "print('AdaBoost: {}'.format(Ada_best.score(X_test, y_test)))\n",
    "print('LightGBM: {}'.format(lgb_best.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn roc_auc: 0.7134567901234568\n",
      "rf roc_auc: 0.7101234567901235\n",
      "lg_reg roc_auc: 0.7358024691358025\n",
      "Extra Tree roc_auc: 0.6911111111111111\n",
      "Adaboost roc_auc: 0.7049382716049383\n",
      "LightGBM roc_auc: 0.6488888888888888\n"
     ]
    }
   ],
   "source": [
    "# Calculating the predicted values of y with the  models;\n",
    "y_pred_knn=knn_best.predict(X_test)\n",
    "y_pred_rf=rf_best.predict(X_test)\n",
    "y_pred_lg=lg_best.predict(X_test)\n",
    "y_pred_ET=ET_best.predict(X_test)\n",
    "y_pred_Ada=Ada_best.predict(X_test)\n",
    "y_pred_lgb=lgb_best.predict(X_test)\n",
    "\n",
    "#Printing the ROC AUC values of the seperate models:\n",
    "print('knn roc_auc: {}'.format(roc_auc_score(y_test,y_pred_knn)))\n",
    "print('rf roc_auc: {}'.format(roc_auc_score(y_test,y_pred_rf)))\n",
    "print('lg_reg roc_auc: {}'.format(roc_auc_score(y_test,y_pred_lg)))\n",
    "print('Extra Tree roc_auc: {}'.format(roc_auc_score(y_test,y_pred_ET)))\n",
    "print('Adaboost roc_auc: {}'.format(roc_auc_score(y_test,y_pred_Ada)))\n",
    "print('LightGBM roc_auc: {}'.format(roc_auc_score(y_test,y_pred_lgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble: 0.7020987654320988\n"
     ]
    }
   ],
   "source": [
    "### The Stacked Ensemble:\n",
    "#Creating a dictionnary of the  models:\n",
    "estimators=[('knn', knn_best), ('rf', rf_best), ('log_reg', lg_best),('Extra tree', ET_best), ('Adaboost',Ada_best), \n",
    "           ('lightGBM',lgb_best)]\n",
    "\n",
    "#using the voting classifier to stack the models:\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "#fitting the new stacked model:\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "#testing the stacked model against the test set:\n",
    "ensemble.score(X_test, y_test)\n",
    "y_pred_ensemble=ensemble.predict(X_test)\n",
    "#Printing the ROC AUC value for the new stacked model:\n",
    "print('ensemble: {}'.format(roc_auc_score(y_test,y_pred_ensemble)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other way to Ensemble :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 1, 'knn__p': 1, 'knn__weights': 'uniform', 'lg__C': 9, 'lg__max_iter': 200, 'rf__max_leaf_nodes': 50, 'rf__n_estimators': 25}\n",
      "{'knn__n_neighbors': 1, 'knn__p': 1, 'knn__weights': 'uniform', 'lg__C': 9, 'lg__max_iter': 200, 'rf__max_leaf_nodes': 50, 'rf__n_estimators': 25}\n"
     ]
    }
   ],
   "source": [
    "### An other way to ensembling / Hyper Parameer tunning\n",
    "voting = VotingClassifier(estimators=[('knn',KNeighborsClassifier() ), \n",
    "                                      ('rf', RandomForestClassifier()),('lg',LogisticRegression())],\n",
    "                                      voting='hard')\n",
    "\n",
    "#Use the key for the classifier followed by __ and the attribute\n",
    "params = {'knn__n_neighbors': np.arange(1, 20,50),\n",
    "          'knn__weights':['distance','uniform'],\n",
    "          'knn__p':[1,2],\n",
    "          'rf__max_leaf_nodes':[10,20,50],          \n",
    "          'rf__n_estimators': [50,25],\n",
    "           'lg__C':  [0.01, 0.1, 1,2,6,9],\n",
    "           'lg__max_iter':[50,100,200]\n",
    "         }\n",
    "\n",
    "Voting_grid = GridSearchCV(estimator=voting, param_grid=params, cv=15,n_jobs=-1)\n",
    "\n",
    "Voting_grid.fit(X_train,y_train)\n",
    "\n",
    "print (Voting_grid.best_params_)\n",
    "\n",
    "#saving the best model:\n",
    "VT_best = Voting_grid.best_estimator_\n",
    "\n",
    "#check best n_estimators value\n",
    "print(Voting_grid.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble: 0.7424691358024691\n"
     ]
    }
   ],
   "source": [
    "y_pred_Voting=VT_best.predict(X_test)\n",
    "\n",
    "print('ensemble: {}'.format(roc_auc_score(y_test,y_pred_Voting)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
